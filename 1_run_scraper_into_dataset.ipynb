{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63053a90",
   "metadata": {},
   "source": [
    "# Automated Car Image Scraping, Validation, and Dataset Preparation Workflow\n",
    "\n",
    "This notebook provides a streamlined workflow for building a high-quality car image dataset, ready for machine learning tasks. The process is fully automated and includes the following steps:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Automated Scraping**  \n",
    "Collect car images from multiple online sources with non-duplicate URLs.\n",
    "\n",
    "- **Current Sources:**  \n",
    "    - Google Images  \n",
    "    - Carmudi  \n",
    "    - Mobil123  \n",
    "\n",
    "---\n",
    "\n",
    "**2. Image Validation & Cropping**  \n",
    "Checks each image for quality and consistency, cropping them as needed to standardize the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Dataset Splitting**  \n",
    "Organizes validated images into training, validation, and test sets for machine learning workflows.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Comprehensive Reporting**  \n",
    "Generates visual and tabular reports on:\n",
    "\n",
    "- Scraping performance  \n",
    "- Validation status  \n",
    "- Class distribution  \n",
    "\n",
    "---\n",
    "\n",
    "This workflow ensures you have a clean, well-organized, and well-documented car image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00144afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install playwright for Browser scraping\n",
    "\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants based on the configuration file\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATA_PROCESSING_DIRECTORY = \"./data_processing\"\n",
    "BASE_DIRECTORY = os.path.join(DATA_PROCESSING_DIRECTORY, \"scraping\")\n",
    "CONFIG_PATH = os.path.join(BASE_DIRECTORY, \"config.json\")\n",
    "\n",
    "\n",
    "OUTPUT_DIRECTORY = os.path.join(DATA_PROCESSING_DIRECTORY, \"data\")\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "REPORTS_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, \"reports\")\n",
    "os.makedirs(REPORTS_DIRECTORY, exist_ok=True)\n",
    "\n",
    "IMAGES_DIRECTORY = os.path.join(OUTPUT_DIRECTORY, \"images\")\n",
    "os.makedirs(IMAGES_DIRECTORY, exist_ok=True)\n",
    "\n",
    "MASTER_CSV_PATH = os.path.join(REPORTS_DIRECTORY, \"master_scraper_results.csv\")\n",
    "SUMMARY_CSV_PATH = os.path.join(REPORTS_DIRECTORY, \"scrape_summary_report.csv\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"⚙️ Configuration loaded. Ready to run the scraper in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "\n",
    "from data_processing.scraping.master_scraper import MasterScraper\n",
    "\n",
    "\n",
    "print(\"Starting the scraping and downloading process...\")\n",
    "scraper = MasterScraper(\n",
    "    config_path=CONFIG_PATH,\n",
    "    images_dir=IMAGES_DIRECTORY,\n",
    "    csv_path=MASTER_CSV_PATH,\n",
    "    summary_csv_path=SUMMARY_CSV_PATH,\n",
    ")\n",
    "await scraper.run()\n",
    "\n",
    "print(\"\\n✅ Scraping and downloading process finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ac4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.scraping.image_downloader import ImageDownloader\n",
    "\n",
    "print(\"--- Starting Image Download Process ---\")\n",
    "\n",
    "downloader = ImageDownloader(csv_path=MASTER_CSV_PATH)\n",
    "await downloader.download_all_images()\n",
    "\n",
    "print(\"\\n✅ Image downloading process finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and crop images\n",
    "\n",
    "from data_processing.scraping.dataset_validator import DatasetValidator\n",
    "\n",
    "REJECTED_DIR = os.path.join(OUTPUT_DIRECTORY, \"rejected_images\")\n",
    "os.makedirs(REJECTED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Starting dataset validation...\")\n",
    "validator = DatasetValidator(\n",
    "    master_csv_path=MASTER_CSV_PATH,\n",
    "    rejected_dir=REJECTED_DIR,\n",
    "    min_resolution=(200, 200),\n",
    ")\n",
    "validator.validate_and_crop_images()\n",
    "\n",
    "print(\"\\n✅ Validation and cropping process finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "\n",
    "import os\n",
    "\n",
    "from data_processing.scraping.create_dataset_split import DatasetSplitter\n",
    "\n",
    "print(\"Starting Dataset Splitting\")\n",
    "\n",
    "FINAL_DATASET_DIR = os.path.join(OUTPUT_DIRECTORY, \"dataset\")\n",
    "\n",
    "splitter = DatasetSplitter(\n",
    "    source_dir=IMAGES_DIRECTORY,\n",
    "    output_dir=FINAL_DATASET_DIR,\n",
    ")\n",
    "splitter.split_and_copy()\n",
    "\n",
    "print(\"\\n✅ Dataset splitting process finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports for scraping performance and validation results\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "\n",
    "def load_all_data(config_file=\"config.json\"):\n",
    "    \"\"\"Loads config and all necessary report CSVs.\"\"\"\n",
    "    summary_df, master_df = None, None\n",
    "    try:\n",
    "\n",
    "        if os.path.exists(SUMMARY_CSV_PATH):\n",
    "            summary_df = pd.read_csv(SUMMARY_CSV_PATH)\n",
    "        else:\n",
    "            print(\n",
    "                f\"❌ Scrape summary file not found at {SUMMARY_CSV_PATH}. Please run Cell 2.\"\n",
    "            )\n",
    "\n",
    "        if os.path.exists(MASTER_CSV_PATH):\n",
    "            master_df = pd.read_csv(MASTER_CSV_PATH)\n",
    "        else:\n",
    "            print(\n",
    "                f\"❌ Master data file not found at {MASTER_CSV_PATH}. Please run Cell 3.\"\n",
    "            )\n",
    "\n",
    "        return summary_df, master_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ {config_file} not found. Please run Cell 1 to create it.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def plot_scraping_performance(df, reports_path):\n",
    "    \"\"\"Generates and displays a bar chart for unique vs. duplicate URLs.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PART 1: SCRAPING PERFORMANCE REPORT (UNIQUES vs. DUPLICATES)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Unique URLs Found vs. Duplicates Skipped per Source:\")\n",
    "    display(df)\n",
    "\n",
    "    df.plot(\n",
    "        x=\"Source\",\n",
    "        y=[\"unique_found\", \"duplicates_skipped\"],\n",
    "        kind=\"bar\",\n",
    "        figsize=(10, 7),\n",
    "        color=[\"#007ACC\", \"#FDCF33\"],\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    plt.title(\n",
    "        \"Scraper Performance: Unique vs. Duplicate URLs Found\",\n",
    "        fontsize=16,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.ylabel(\"Number of Image URLs\", fontsize=12)\n",
    "    plt.xlabel(\"Source Website\", fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(reports_path, \"chart_scraping_performance.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_validation_pie_chart(df, reports_path):\n",
    "    \"\"\"Generates a pie chart of the overall validation status.\"\"\"\n",
    "    print(\"\\n--- 2a. Overall Validation Status ---\")\n",
    "    status_counts = df[\"validation_status\"].value_counts()\n",
    "    display(status_counts)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    status_counts.plot.pie(\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        wedgeprops={\"edgecolor\": \"black\", \"linewidth\": 1},\n",
    "        textprops={\"fontsize\": 12},\n",
    "    )\n",
    "    plt.title(\"Breakdown of Final Image Status\", fontsize=16, weight=\"bold\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(reports_path, \"chart_validation_status.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution_chart(df, reports_path):\n",
    "    \"\"\"Generates a bar chart showing the number of valid images per class.\"\"\"\n",
    "    print(\"\\n--- 2b. Final Class Distribution ---\")\n",
    "    class_counts = df[\"class\"].value_counts()\n",
    "    display(class_counts)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    class_counts.plot(kind=\"bar\", color=\"#007ACC\", edgecolor=\"black\")\n",
    "    plt.title(\"Number of Valid Images per Class\", fontsize=16, weight=\"bold\")\n",
    "    plt.ylabel(\"Number of Images\", fontsize=12)\n",
    "    plt.xlabel(\"Car Class\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(reports_path, \"chart_class_distribution.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_all_reports():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire reporting process, including\n",
    "    scraping performance and validation results.\n",
    "    \"\"\"\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "    summary_df, master_df = load_all_data()\n",
    "\n",
    "    if summary_df is not None:\n",
    "        plot_scraping_performance(summary_df, REPORTS_DIRECTORY)\n",
    "\n",
    "    if master_df is not None and \"validation_status\" in master_df.columns:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PART 2: VALIDATION & FINAL DATASET REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        valid_images_df = master_df[\n",
    "            master_df[\"validation_status\"] == \"valid_and_cropped\"\n",
    "        ].copy()\n",
    "\n",
    "        plot_validation_pie_chart(master_df, REPORTS_DIRECTORY)\n",
    "\n",
    "        if not valid_images_df.empty:\n",
    "            plot_class_distribution_chart(valid_images_df, REPORTS_DIRECTORY)\n",
    "        else:\n",
    "            print(\"\\nNo valid images found to generate final dataset reports.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping validation reports because data file was not found or not validated.\")\n",
    "\n",
    "\n",
    "generate_all_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
